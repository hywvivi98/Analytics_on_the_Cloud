{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">GraphFrames Assignment</span>\n",
    "<br><br>\n",
    "In this assignment, you need to do the following:\n",
    "<li>Read the file 201710-citibike-tripdata.csv</li>\n",
    "<li>Construct a graph with stations as vertices and trips between stations as edges</li>\n",
    "<li>Vertex Ids are station numbers and Vertex attributes are station names</li>\n",
    "<li>Edge attributes are trip duration (durations are in seconds)</li>\n",
    "<li>Then answer the questions below</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages= [\"graphframes:graphframes:0.8.2-spark3.2-s_2.12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.2:4041\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = local[*], app id = local-1649035831441)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.graphframes._\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//GraphFrame imports\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.graphframes._\n",
    "\n",
    "\n",
    "//GraphX imports\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Step 1: Construct the graph</span>\n",
    "<br><br>\n",
    "<li>read the data file and drop the header line</li>\n",
    "<li>create a vertex rdd (the union of start stations and end stations</li>\n",
    "<li>create an edge rdd (the trips - start station id, end station id, duration</li>\n",
    "<li>create a graph</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = 201710-citibike-tripdata.csv MapPartitionsRDD[1] at textFile at <console>:37\n",
       "data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at mapPartitionsWithIndex at <console>:39\n",
       "start: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[4] at map at <console>:41\n",
       "end: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[6] at map at <console>:42\n",
       "vertices: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[10] at distinct at <console>:43\n",
       "edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[12] at map at <console>:45\n",
       "g: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@229c60c4\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"201710-citibike-tripdata.csv\")\n",
    "// drop the header line\n",
    "val data = text.mapPartitionsWithIndex{ (idx,iter) => if (idx==0) iter.drop(1) else iter}\n",
    "// create vertex rdd\n",
    "val start = data.map(r => r.split(\",\")).map(r => (r(3).toLong,r(4)))\n",
    "val end = data.map(r => r.split(\",\")).map(r => (r(7).toLong,r(8)))\n",
    "val vertices =(start union end ).distinct\n",
    "// create edge rdd\n",
    "val edges = data.map(r => r.split(\",\")).map(r => Edge(r(3).toLong,r(7).toLong,r(0).toInt))\n",
    "// create a graph\n",
    "val g: Graph[String, Int] = Graph(vertices, edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Step 2: Basic questions</span>\n",
    "<br><br>\n",
    "\n",
    "<li>How many citibike stations are there in the network?</li>\n",
    "<li>How many trips were made in the month in question?</li>\n",
    "<li>How many trips started and ended at the same station?</li>\n",
    "<li>How many station to station connections are there (at least one edge exists between station i and station j and i is not equal to j)?</li>\n",
    "<li>Your code should print:</li>\n",
    "<pre>\n",
    "Total number of stations: 785\n",
    "Total number of trips.  : 1897592\n",
    "Trips that started and ended at the same station: 33245\n",
    "Number of station to station connections: 107524\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_undirected_graph: (g: org.graphframes.GraphFrame)org.graphframes.GraphFrame\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//You might need this\n",
    "def make_undirected_graph(g: GraphFrame) = {\n",
    "    val u_edge_df = g.find(\"(a)-[]->(b)\")\n",
    "        .select($\"a.id\".as(\"src\"),$\"b.id\".as(\"dst\"))\n",
    "        .withColumn(\"swap\",when(col(\"src\")<col(\"dst\"),col(\"dst\")))\n",
    "        .withColumn(\"dst\",\n",
    "                    when(col(\"swap\").isNotNull,col(\"src\"))\n",
    "                    .otherwise(col(\"dst\")))\n",
    "        .withColumn(\"src\",\n",
    "                    when(col(\"swap\").isNotNull,col(\"swap\"))\n",
    "                   .otherwise(col(\"src\")))\n",
    "        .drop(col(\"swap\"))\n",
    "        .distinct\n",
    "    val u_vertices_df = g.vertices\n",
    "    val u_g = GraphFrame(u_vertices_df,u_edge_df)    \n",
    "    u_g\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station_number: Long = 785\n",
       "trip_number: Long = 1897592\n",
       "same_station: Long = 33245\n",
       "stat_to_stat: Long = 107524\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val station_number = g.vertices.count\n",
    "val trip_number = g.edges.count\n",
    "val same_station = g.edges.filter(r => r.srcId == r.dstId).count\n",
    "val stat_to_stat = g.convertToCanonicalEdges().edges.filter(r => r.srcId != r.dstId).count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "// import org.apache.spark.graphx.Graph\n",
    "// import org.apache.spark.sql.Row\n",
    "// val g2: GraphFrame = GraphFrame.fromGraphX(g)\n",
    "// make_undirected_graph(g2).edges.distinct.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Step 3: Find the Station from which most trips originate</span>\n",
    "<br><br>\n",
    "<li>Note that the graph has one edge for each trip (i.e., there are many edges between two vertices)</li>\n",
    "<li>The function <span style=\"color:blue\">outDegrees</span> returns the number of outgoing edges from every vertex</li>\n",
    "<li>Print the name of the station with most originating trips</li>\n",
    "<li>Your code should print:</li>\n",
    "<pre>  \n",
    "The station from which most trips originate is: \"Pershing Square North\"\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ljava.lang.Object;@5d86c187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx.Graph\n",
       "import org.apache.spark.sql.Row\n",
       "g2: org.graphframes.GraphFrame = GraphFrame(v:[id: bigint, attr: string], e:[src: bigint, dst: bigint ... 1 more field])\n",
       "mapped_id: Array[Any] = Array(519)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// method1 -- use Graph\n",
    "import org.apache.spark.graphx.Graph\n",
    "import org.apache.spark.sql.Row\n",
    "val g2: GraphFrame = GraphFrame.fromGraphX(g)\n",
    "// find the id number\n",
    "val mapped_id = g2.outDegrees.orderBy(desc(\"outDegree\")).take(1).map(r=> r(0))\n",
    "println(mapped_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Pershing Square North\"\n"
     ]
    }
   ],
   "source": [
    "// select name for that corresponding id\n",
    "println(g2.vertices.filter(\"id=519\").select(\"attr\").first.getString(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Pershing Square North\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "out_most: (org.apache.spark.graphx.VertexId, Int) = (519,17995)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// method2 -- use GraphX\n",
    "val out_most = g.outDegrees.collect.sortBy(_._2).reverse(0)\n",
    "g.vertices.filter(r => r._1 == out_most._1).foreach(r => println(r._2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">STEP 4: Proportion of trips for each station that start and end at that same station</span>\n",
    "<br><br>\n",
    "<li>Create a GraphX graph from the GraphFrames graph (use the method that retains datatypes)</li>\n",
    "<li>Use aggregateMessages to calculate the number of trips that start and end at the same vertex (for each vertex)</li>\n",
    "<li>Convert the resulting (VertexRDD) to a DataFrame</li>\n",
    "<li>Using join (and select), add the location of the station column to the df</li>\n",
    "<li>Join this df to the out degrees df created earlier</li>\n",
    "<li>Divide the same trips column by the out degrees column and select the appropriate rows</li>\n",
    "<li>Sort the resulting df by this proportion in descending order</li>\n",
    "<li>Your output should be the following dataframe</li>\n",
    "<li>Note: Though you can use aggregateMessages in GraphFrames, you must use the GraphX version for this assignment</li>\n",
    "    \n",
    "<pre>\n",
    "+----+--------------------+-----+---------+-------------------+\n",
    "|  id|            location|trips|outDegree|               prop|\n",
    "+----+--------------------+-----+---------+-------------------+\n",
    "|3488|  \"8D QC Station 01\"|    1|        1|                1.0|\n",
    "|3245|\"NYCBS DEPOT - DE...|    1|        2|                0.5|\n",
    "|3182|\"Yankee Ferry Ter...|  309|      900| 0.3433333333333333|\n",
    "|3254|  \"Soissons Landing\"|  358|     1100|0.32545454545454544|\n",
    "|3342|\"Pioneer St & Ric...|   59|      299|0.19732441471571907|\n",
    "|3477|\"39 St & 2 Ave - ...|   45|      245| 0.1836734693877551|\n",
    "|3532|\"Ditmars Blvd & 1...|   70|      407|  0.171990171990172|\n",
    "|3180|\"Brooklyn Bridge ...|  232|     1354|0.17134416543574593|\n",
    "|3423|\"West Drive & Pro...|  367|     2463|0.14900527811611855|\n",
    "|3636|\"Expansion Wareho...|    1|        8|              0.125|\n",
    "|3302|\"Columbus Ave & W...|   74|      598|0.12374581939799331|\n",
    "|3120|\"Center Blvd & Bo...|   74|      622| 0.1189710610932476|\n",
    "|3514|\"Astoria Park S &...|   34|      299|0.11371237458193979|\n",
    "|3479|      \"Picnic Point\"|   77|      712|0.10814606741573034|\n",
    "|3594|\"Montgomery St & ...|    9|       87|0.10344827586206896|\n",
    "|3524|    \"19 St & 24 Ave\"|   24|      249| 0.0963855421686747|\n",
    "|3349|\"Grand Army Plaza...|  237|     2570|0.09221789883268483|\n",
    "|3333|\"Columbia St & Lo...|    5|       55|0.09090909090909091|\n",
    "|3354|\"3 St & Prospect ...|  142|     1572|0.09033078880407125|\n",
    "|3607|    \"31 Ave & 14 St\"|   10|      113|0.08849557522123894|\n",
    "+----+--------------------+-----+---------+-------------------+\n",
    "only showing top 20 rows\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val v = g.vertices.rdd.map(r => (r(0).toString.toLong,r(1).toString))\n",
    "// val e = g.edges.rdd.map(r => Edge(r(0).toString.toLong,r(1).toString.toLong,r(2).toString.toDouble))\n",
    "// val gx: Graph[String, Double] = Graph(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+---------+-------------------+\n",
      "|  id|            location|trips|outDegree|               prop|\n",
      "+----+--------------------+-----+---------+-------------------+\n",
      "|3488|  \"8D QC Station 01\"|    1|        1|                1.0|\n",
      "|3245|\"NYCBS DEPOT - DE...|    1|        2|                0.5|\n",
      "|3182|\"Yankee Ferry Ter...|  309|      900| 0.3433333333333333|\n",
      "|3254|  \"Soissons Landing\"|  358|     1100|0.32545454545454544|\n",
      "|3342|\"Pioneer St & Ric...|   59|      299|0.19732441471571907|\n",
      "|3477|\"39 St & 2 Ave - ...|   45|      245| 0.1836734693877551|\n",
      "|3532|\"Ditmars Blvd & 1...|   70|      407|  0.171990171990172|\n",
      "|3180|\"Brooklyn Bridge ...|  232|     1354|0.17134416543574593|\n",
      "|3423|\"West Drive & Pro...|  367|     2463|0.14900527811611855|\n",
      "|3636|\"Expansion Wareho...|    1|        8|              0.125|\n",
      "|3302|\"Columbus Ave & W...|   74|      598|0.12374581939799331|\n",
      "|3120|\"Center Blvd & Bo...|   74|      622| 0.1189710610932476|\n",
      "|3514|\"Astoria Park S &...|   34|      299|0.11371237458193979|\n",
      "|3479|      \"Picnic Point\"|   77|      712|0.10814606741573034|\n",
      "|3594|\"Montgomery St & ...|    9|       87|0.10344827586206896|\n",
      "|3524|    \"19 St & 24 Ave\"|   24|      249| 0.0963855421686747|\n",
      "|3349|\"Grand Army Plaza...|  237|     2570|0.09221789883268483|\n",
      "|3333|\"Columbia St & Lo...|    5|       55|0.09090909090909091|\n",
      "|3354|\"3 St & Prospect ...|  142|     1572|0.09033078880407125|\n",
      "|3607|    \"31 Ave & 14 St\"|   10|      113|0.08849557522123894|\n",
      "+----+--------------------+-----+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tripNum_sameVer: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[66] at RDD at VertexRDD.scala:57\n",
       "tripNum_df: org.apache.spark.sql.DataFrame = [id_trip: bigint, trips: int]\n",
       "stat_name: org.apache.spark.sql.DataFrame = [id: bigint, location: string]\n",
       "joined_name: org.apache.spark.sql.DataFrame = [id_trip: bigint, trips: int ... 2 more fields]\n",
       "stat_degree: org.apache.spark.sql.DataFrame = [id: bigint, outDegree: int]\n",
       "joined_degree: org.apache.spark.sql.DataFrame = [trips: int, location: string ... 2 more fields]\n",
       "joined_withprop: org.apache.spark.sql.DataFrame = [trips: int, location: string ... 3 more fields]\n",
       "final_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, location: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// calculate the number of trips\n",
    "val tripNum_sameVer = g.aggregateMessages[Int](ec => {if (ec.srcId==ec.dstId) ec.sendToDst(1) else ec.sendToDst(0)},\n",
    "                                                   (x,y) => x+y)\n",
    "// Convert the resulting (VertexRDD) to a DataFrame\n",
    "val tripNum_df = spark.createDataFrame(tripNum_sameVer).toDF(\"id_trip\",\"trips\")\n",
    "// add the location of the station column to the df\n",
    "val stat_name = spark.createDataFrame(g.vertices).toDF(\"id\",\"location\")\n",
    "val joined_name = tripNum_df.select(\"id_trip\",\"trips\").join(stat_name,tripNum_df(\"id_trip\")===stat_name(\"id\"))\n",
    "// Join this df to the out degrees df created earlier\n",
    "val stat_degree = spark.createDataFrame(g.outDegrees).toDF(\"id\",\"outDegree\")\n",
    "val joined_degree = joined_name.select(\"id_trip\",\"trips\",\"location\").join(stat_degree,joined_name(\"id_trip\")===stat_degree(\"id\")).drop(\"id_trip\")\n",
    "// Divide the same trips column by the out degrees column\n",
    "val joined_withprop =joined_degree.withColumn(\"prop\",col(\"trips\")/col(\"outDegree\"))\n",
    "// Sort the resulting df by this proportion in descending order\n",
    "val final_df = joined_withprop.select(\"id\",\"location\",\"trips\",\"outDegree\",\"prop\").orderBy(desc(\"prop\"))\n",
    "final_df.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">STEP 5: Create a new graph that contains all edges except for those between the same station</span>\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_g: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@6ff5b88f\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val new_g = g.subgraph(ec => ec.srcId != ec.dstId, (v_id,v_attr) => true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">STEP 6: Calculate the average duration between every pair of stations</span>\n",
    "<br><br>\n",
    "<li>use the new graph from step 5 for this</li>\n",
    "<li>I'll let you figure this out but this should be really easy (think SQL)</li>\n",
    "<pre>\n",
    "+----+----+------------------+\n",
    "| src| dst|                 m|\n",
    "+----+----+------------------+\n",
    "| 504| 350| 772.7647058823529|\n",
    "| 433| 527| 532.9677419354839|\n",
    "| 434| 470|316.52272727272725|\n",
    "| 438| 151|  546.195652173913|\n",
    "| 445| 507| 553.3947368421053|\n",
    "|2021| 446| 827.6904761904761|\n",
    "| 116| 518| 1115.857142857143|\n",
    "|3435| 358| 895.6666666666666|\n",
    "|3402|3414| 634.1666666666666|\n",
    "| 498| 495| 801.2272727272727|\n",
    "|3637| 418|             889.7|\n",
    "| 380|3260|419.42105263157896|\n",
    "|3360| 507|            1442.0|\n",
    "| 326| 247|             713.0|\n",
    "|3358| 467| 500.6666666666667|\n",
    "|3164| 457|502.97241379310344|\n",
    "| 498| 528| 434.3207547169811|\n",
    "| 405|3256| 843.2168674698795|\n",
    "| 477|2000|2672.3333333333335|\n",
    "|3226|3163| 686.4444444444445|\n",
    "+----+----+------------------+\n",
    "only showing top 20 rows\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------------+\n",
      "|src| dst|                 m|\n",
      "+---+----+------------------+\n",
      "| 72| 484| 627.6666666666666|\n",
      "|116| 504| 712.1111111111111|\n",
      "|116| 518| 1115.857142857143|\n",
      "|120| 410|            1498.2|\n",
      "|127| 485|           1333.25|\n",
      "|127| 486|            855.25|\n",
      "|127|3378|1789.3333333333333|\n",
      "|150| 433|           2071.28|\n",
      "|151| 116|             913.0|\n",
      "|157|3419| 592.6666666666666|\n",
      "|161| 531| 594.4285714285714|\n",
      "|164|3443| 555.1081081081081|\n",
      "|173|3295|            1348.6|\n",
      "|195| 253|            1214.8|\n",
      "|195| 355| 825.3793103448276|\n",
      "|195| 442|            1314.0|\n",
      "|228| 509|             984.2|\n",
      "|229|3093|            1405.7|\n",
      "|236| 253|             497.0|\n",
      "|236| 341| 911.6363636363636|\n",
      "+---+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pair_df: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]\n",
       "pair_mean: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pair_df = new_g.edges.map(e => (e.srcId.toInt,e.dstId.toInt,e.attr)).toDF(\"src\",\"dst\",\"attr\")\n",
    "val pair_mean = pair_df.groupBy(\"src\",\"dst\").agg(avg(\"attr\").as(\"m\"))\n",
    "// test with result, compare with the given case\n",
    "// pair_mean.filter(pair_mean(\"src\")===433 && pair_mean(\"dst\")=== 527).show\n",
    "\n",
    "pair_mean.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">STEP 7: Important stations</span><br><br>\n",
    "Citibike wants to figure out how best to deploy its workers in checking whether a station is over-full (too many bikes) or needs more bikes. It figures that the best way to do this is to find out which stations are the most important in terms of flows:\n",
    "<li>A station that has high bike returns and is connected to other stations with high bike returns is more likely to have too many bikes in its station and therefore should be monitored more often</li>\n",
    "<li>A station that has high bike pickups and is connected to other stations with high bike pickups is more likely to be short of bikes and therefore should be monitored more often</li>\n",
    "<li>Calculate the propensities for over-fullness and emptiness for every station</li>\n",
    "<li>Report the 5 most important stations for over-fullness (use pageRank on the graph)</li>\n",
    "<li>Report the 5 most important stations for emptiness (reverse all the edges on the graph and use pageRank)</li>\n",
    "<li>Your results (Don't worry about the meaning of location names!):</li>\n",
    "<li>Note: Assume a reset_probability of 0.15 and a tolerance of .0001 if you want the same results as mine</li>\n",
    "<pre>\n",
    "+---+--------------------+------------------+\n",
    "| id|            location|          pagerank|\n",
    "+---+--------------------+------------------+\n",
    "|519|\"Pershing Square ...| 4.930887390071603|\n",
    "|426|\"West St & Chambe...|3.7410934274030576|\n",
    "|402|\"Broadway & E 22 St\"|  3.58520147183096|\n",
    "|497|\"E 17 St & Broadway\"| 3.537658018512581|\n",
    "|435|   \"W 21 St & 6 Ave\"| 3.438585855241344|\n",
    "+---+--------------------+------------------+\n",
    "\n",
    "+----+--------------------+------------------+\n",
    "|  id|            location|          pagerank|\n",
    "+----+--------------------+------------------+\n",
    "|3197|      \"Hs Don't Use\"| 5.710640869520747|\n",
    "| 519|\"Pershing Square ...| 5.012823444592195|\n",
    "|3480|      \"WS Don't Use\"| 4.272284643284593|\n",
    "| 402|\"Broadway & E 22 St\"|3.4515211069038183|\n",
    "| 497|\"E 17 St & Broadway\"|3.3347259745457443|\n",
    "+----+--------------------+------------------+\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+\n",
      "| id|                attr|          pagerank|\n",
      "+---+--------------------+------------------+\n",
      "|519|\"Pershing Square ...| 4.930887390071603|\n",
      "|426|\"West St & Chambe...|3.7410934274030576|\n",
      "|402|\"Broadway & E 22 St\"|  3.58520147183096|\n",
      "|497|\"E 17 St & Broadway\"| 3.537658018512581|\n",
      "|435|   \"W 21 St & 6 Ave\"| 3.438585855241344|\n",
      "+---+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "over_full: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, attr: string ... 1 more field]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val over_full = g2.pageRank.resetProbability(0.15).tol(0.0001).run()\n",
    "                    .vertices.orderBy(desc(\"pagerank\"))\n",
    "over_full.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reversed_edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[988] at map at <console>:41\n",
       "reversed_g: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@699030f5\n",
       "reversed_g2: org.graphframes.GraphFrame = GraphFrame(v:[id: bigint, attr: string], e:[src: bigint, dst: bigint ... 1 more field])\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//reverse all the edges on the graph, generate new graphframe\n",
    "val reversed_edges = data.map(r => r.split(\",\")).map(r => Edge(r(7).toLong,r(3).toLong,r(0).toInt))\n",
    "val reversed_g: Graph[String, Int] = Graph(vertices, reversed_edges)\n",
    "val reversed_g2: GraphFrame = GraphFrame.fromGraphX(reversed_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+\n",
      "|  id|                attr|          pagerank|\n",
      "+----+--------------------+------------------+\n",
      "|3197|      \"Hs Don't Use\"| 5.710640869520747|\n",
      "| 519|\"Pershing Square ...| 5.012823444592195|\n",
      "|3480|      \"WS Don't Use\"| 4.272284643284593|\n",
      "| 402|\"Broadway & E 22 St\"|3.4515211069038183|\n",
      "| 497|\"E 17 St & Broadway\"|3.3347259745457443|\n",
      "+----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emptiness: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, attr: string ... 1 more field]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val emptiness = reversed_g2.pageRank.resetProbability(0.15).tol(0.0001).run()\n",
    "                    .vertices.orderBy(desc(\"pagerank\"))\n",
    "emptiness.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">STEP 8: Calculate the clustering coefficient of every station</span><br><br>\n",
    "\n",
    "<li>And report the top 20 stations by clustering coefficient</li>\n",
    "<pre>\n",
    "+----+--------------------+------------------+\n",
    "|  id|            location|             coeff|\n",
    "+----+--------------------+------------------+\n",
    "|3040|     \"GOW Tech Shop\"|               1.0|\n",
    "|3639|        \"Harborside\"|               1.0|\n",
    "|3192|\"Liberty Light Rail\"|               1.0|\n",
    "|3485| \"NYCBS Depot - RIS\"|               1.0|\n",
    "|3647|    \"48 Ave & 30 Pl\"|               1.0|\n",
    "|3279|       \"Dixon Mills\"|               1.0|\n",
    "|3186|     \"Grove St PATH\"|               1.0|\n",
    "| 153|   \"E 40 St & 5 Ave\"|               1.0|\n",
    "| 339|\"Avenue D & E 12 St\"| 0.877201420748853|\n",
    "|3464|\"W 37 St & Broadway\"|0.8679573382796197|\n",
    "| 247|\"Perry St & Bleec...|0.8602079768329604|\n",
    "|3175|\"W 70 St & Amster...|0.8592469808193227|\n",
    "|3176|\"W 64 St & West E...|0.8568452539928423|\n",
    "|3623|\"W 120 St & Clare...|0.8549019607843137|\n",
    "|3491|  \"E 118 St & 1 Ave\"| 0.854122621564482|\n",
    "| 266| \"Avenue D & E 8 St\"| 0.849218980253463|\n",
    "|3441|   \"10 Hudson Yards\"|0.8482701509017299|\n",
    "|3646|    \"35 Ave & 10 St\"|0.8333333333333334|\n",
    "|3642|\"E 98 St & Lexing...|             0.832|\n",
    "| 444|\"Broadway & W 24 St\"|0.8283229697508064|\n",
    "+----+--------------------+------------------+\n",
    "only showing top 20 rows\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "triangles: org.apache.spark.sql.DataFrame = [count: bigint, t_id: bigint ... 1 more field]\n",
       "degrees: org.apache.spark.sql.DataFrame = [id: bigint, degree: int]\n",
       "possible: org.apache.spark.sql.DataFrame = [id: bigint, degree: int ... 1 more field]\n",
       "joined: org.apache.spark.sql.DataFrame = [t_id: bigint, count: bigint ... 4 more fields]\n",
       "coeff: org.apache.spark.sql.DataFrame = [t_id: bigint, count: bigint ... 5 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val triangles = g2.triangleCount.run().withColumnRenamed(\"id\",\"t_id\") //Get the number of triangles each vertex belongs to\n",
    "val degrees = make_undirected_graph(g2).degrees //Get the number of adjacent vertices for each vertex\n",
    "val possible = degrees.withColumn(\"possible\",col(\"degree\")*(col(\"degree\")-1)/lit(2)) //Calculate possible triangles\n",
    "val joined = triangles.select(\"t_id\",\"count\",\"attr\").join(possible,triangles(\"t_id\")===possible(\"id\")) \n",
    "val coeff = joined.withColumn(\"coeff\",col(\"count\")/col(\"possible\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+\n",
      "|  id|            location|             coeff|\n",
      "+----+--------------------+------------------+\n",
      "|3279|       \"Dixon Mills\"|               1.0|\n",
      "|3647|    \"48 Ave & 30 Pl\"|               1.0|\n",
      "|3040|     \"GOW Tech Shop\"|               1.0|\n",
      "|3186|     \"Grove St PATH\"|               1.0|\n",
      "| 153|   \"E 40 St & 5 Ave\"|               1.0|\n",
      "|3192|\"Liberty Light Rail\"|               1.0|\n",
      "|3485| \"NYCBS Depot - RIS\"|               1.0|\n",
      "|3639|        \"Harborside\"|               1.0|\n",
      "| 339|\"Avenue D & E 12 St\"| 0.877201420748853|\n",
      "|3464|\"W 37 St & Broadway\"|0.8679573382796197|\n",
      "| 247|\"Perry St & Bleec...|0.8602079768329604|\n",
      "|3175|\"W 70 St & Amster...|0.8592469808193227|\n",
      "|3176|\"W 64 St & West E...|0.8568452539928423|\n",
      "|3623|\"W 120 St & Clare...|0.8549019607843137|\n",
      "|3491|  \"E 118 St & 1 Ave\"| 0.854122621564482|\n",
      "| 266| \"Avenue D & E 8 St\"| 0.849218980253463|\n",
      "|3441|   \"10 Hudson Yards\"|0.8482701509017299|\n",
      "|3646|    \"35 Ave & 10 St\"|0.8333333333333334|\n",
      "|3642|\"E 98 St & Lexing...|             0.832|\n",
      "| 444|\"Broadway & W 24 St\"|0.8283229697508064|\n",
      "+----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fianl_result: org.apache.spark.sql.DataFrame = [id: bigint, location: string ... 1 more field]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fianl_result = coeff.select(\"id\",\"attr\",\"coeff\").orderBy(desc(\"coeff\")).withColumnRenamed(\"attr\",\"location\")\n",
    "fianl_result.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
